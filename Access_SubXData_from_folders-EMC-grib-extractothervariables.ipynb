{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "from datetime import timedelta, datetime, date\n",
    "import calendar\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is my preprocessing function, which I run on each grib file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, preprocess, append, and concatenate all the grib files. In this notebook I ran it on half of the data available.\n",
    "\n",
    "### I will comment the code below for pr_sfc. I repeat the code for each variable separiately. I could loop it for a list of varname =[ 'pr_sfc' , ....] but I preferred keeping it separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# # just have dask figure everything out\n",
    "# client = Client()\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cape\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "tas_2m\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "tdps_sfc\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "wap_500\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "variable_names = ['cape', 'tas_2m', 'tdps_sfc', 'wap_500']\n",
    "\n",
    "def ppf2_filename(ds,filename):\n",
    "    # rename coordinates\n",
    "    ds  = ds.rename({ 'lon_0':'lon', 'lat_0':'lat', 'forecast_time0':'time'})\n",
    "    # getting start time from file name\n",
    "    ts = filename.split('_')\n",
    "#     print(ts)\n",
    "    whereGEFS = ts.index('GEFS')\n",
    "#     print(whereGEFS)\n",
    "    dateSstr = ts[whereGEFS+1]\n",
    "#     print(dateSstr)\n",
    "    dateS = datetime.strptime(dateSstr, '%d%b%Y')\n",
    "#     print(dateS)\n",
    "    \n",
    "    Mvalue = np.int(float(np.asarray(ts[-1][2:4])))\n",
    "\n",
    "    ds.coords['S'] = 'S', np.atleast_1d(dateS)\n",
    "    ds.coords['M'] = 'M', np.atleast_1d(Mvalue)\n",
    "    \n",
    "    ds.coords['time']=np.array([  43200000000000,  129600000000000,  216000000000000,\n",
    "        302400000000000,  388800000000000,  475200000000000,\n",
    "        561600000000000,  648000000000000,  734400000000000,\n",
    "        820800000000000,  907200000000000,  993600000000000,\n",
    "       1080000000000000, 1166400000000000, 1252800000000000,\n",
    "       1339200000000000, 1425600000000000, 1512000000000000,\n",
    "       1598400000000000, 1684800000000000, 1771200000000000,\n",
    "       1857600000000000, 1944000000000000, 2030400000000000,\n",
    "       2116800000000000, 2203200000000000, 2289600000000000,\n",
    "       2376000000000000, 2462400000000000, 2548800000000000,\n",
    "       2635200000000000, 2721600000000000, 2808000000000000,\n",
    "       2894400000000000, 2980800000000000],\n",
    "      dtype='timedelta64[ns]')\n",
    "\n",
    "    # select US\n",
    "    ds = ds.sel(lat=slice(50,23), lon=slice(230,300))\n",
    "#     print(ds)\n",
    "    return ds\n",
    "\n",
    "def read_netcdfs(files, dim, transform_func=None):\n",
    "    def process_one_path(path):\n",
    "        # use a context manager, to ensure the file gets closed after use\n",
    "#         print(path)\n",
    "        with xr.open_dataset(path, engine='pynio', chunks={}) as ds:\n",
    "            # transform_func should do some sort of selection or\n",
    "            # aggregation\n",
    "            if transform_func is not None:\n",
    "                ds = transform_func(ds, path)\n",
    "            # load all data from the transformed dataset, to ensure we can\n",
    "            # use it after closing each original file\n",
    "#             print(ds)\n",
    "#             ds.load()\n",
    "            return ds\n",
    "#     print(files)\n",
    "    paths = sorted(glob(files))\n",
    "#     print(paths)\n",
    "    datasets = [process_one_path(p) for p in paths]\n",
    "    \n",
    "    combined = xr.combine_by_coords(datasets)\n",
    "    combined = combined.chunk({'S':4,'M':-1})\n",
    "    return combined\n",
    "\n",
    "var_l=[]\n",
    "for ivx, ivar in enumerate(variable_names):\n",
    "    print(ivar)\n",
    "    if ivar in os.listdir('/Data2/SubX/EMC/GEFS/'):\n",
    "        c_l = []\n",
    "        for iy in np.arange(1999, 2017):#2017\n",
    "            print(iy)\n",
    "            # here we suppose we only care about the combined mean of each file;\n",
    "            # you might also use indexing operations like .sel to subset datasets\n",
    "            combined = read_netcdfs('/Data2/SubX/EMC/GEFS/'+ivar+'/'+np.str(iy)+'/*/*.grb2', dim=['S','M'],\n",
    "                                    transform_func=ppf2_filename)\n",
    "            \n",
    "            c_l.append(combined)\n",
    "    all_values = xr.concat(c_l, dim='S') \n",
    "    var_l.append(all_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:             (M: 10, S: 939, lat: 28, lon: 71, time: 35)\n",
       "Coordinates:\n",
       "  * lat                 (lat) float32 50.0 49.0 48.0 47.0 ... 25.0 24.0 23.0\n",
       "  * lon                 (lon) float32 230.0 231.0 232.0 ... 298.0 299.0 300.0\n",
       "  * time                (time) timedelta64[ns] 0 days 12:00:00 ... 34 days 12:00:00\n",
       "  * M                   (M) int64 0 1 2 3 4 5 6 7 8 9\n",
       "  * S                   (S) datetime64[ns] 1999-01-06 1999-01-13 ... 2016-12-28\n",
       "Data variables:\n",
       "    CAPE_P1_2L108_GLL0  (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>\n",
       "    TMP_P1_L103_GLL0    (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>\n",
       "    DPT_P1_L103_GLL0    (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>\n",
       "    VVEL_P1_L100_GLL0   (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables = xr.merge(var_l)\n",
    "all_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables = all_variables.rename({'CAPE_P1_2L108_GLL0':'cape',\n",
    "                                      'TMP_P1_L103_GLL0':'tas_2m',\n",
    "                                      'DPT_P1_L103_GLL0':'tdps_2m',\n",
    "                                      'VVEL_P1_L100_GLL0':'wap_500'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (M: 10, S: 939, lat: 28, lon: 71, time: 35)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 50.0 49.0 48.0 47.0 46.0 ... 27.0 26.0 25.0 24.0 23.0\n",
       "  * lon      (lon) float32 230.0 231.0 232.0 233.0 ... 297.0 298.0 299.0 300.0\n",
       "  * time     (time) timedelta64[ns] 0 days 12:00:00 ... 34 days 12:00:00\n",
       "  * M        (M) int64 0 1 2 3 4 5 6 7 8 9\n",
       "  * S        (S) datetime64[ns] 1999-01-06 1999-01-13 ... 2016-12-21 2016-12-28\n",
       "Data variables:\n",
       "    cape     (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>\n",
       "    tas_2m   (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>\n",
       "    tdps_2m  (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>\n",
       "    wap_500  (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (M: 10, S: 939, lat: 28, lon: 71, time: 35)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 50.0 49.0 48.0 47.0 46.0 ... 27.0 26.0 25.0 24.0 23.0\n",
       "  * lon      (lon) float32 230.0 231.0 232.0 233.0 ... 297.0 298.0 299.0 300.0\n",
       "  * time     (time) timedelta64[ns] 0 days 12:00:00 ... 34 days 12:00:00\n",
       "  * M        (M) int64 0 1 2 3 4 5 6 7 8 9\n",
       "  * S        (S) datetime64[ns] 1999-01-06 1999-01-13 ... 2016-12-21 2016-12-28\n",
       "Data variables:\n",
       "    cape     (S, M, time, lat, lon) float32 2.0 4.0 7.0 2.0 ... 39.0 32.0 25.0\n",
       "    tas_2m   (S, M, time, lat, lon) float32 281.35 281.5 ... 296.66998 296.6\n",
       "    tdps_2m  (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>\n",
       "    wap_500  (S, M, time, lat, lon) float32 dask.array<chunksize=(4, 10, 35, 28, 71), meta=np.ndarray>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iv in ['cape', 'tas_2m', 'tdps_2m', 'wap_500']:\n",
    "# for iv in ['tas_2m', 'tdps_2m', 'wap_500']:\n",
    "for iv in ['cape', 'tas_2m','tdps_2m', 'wap_500']:\n",
    "    var_temp = all_variables[[iv]].load()\n",
    "    all_variables_3L = var_temp.rolling(time=3).sum()\n",
    "    all_variables_3L.coords['time'] = all_variables_3L.time.values-np.timedelta64(2,'D')\n",
    "    all_variables_3L = all_variables_3L.isel(time=slice(2,36))\n",
    "    all_variables_3L.coords['target_time'] = all_variables_3L.S+all_variables_3L.time\n",
    "    all_variables_3L.to_netcdf('all_variables_3L_oct2020_'+iv+'.nc')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
