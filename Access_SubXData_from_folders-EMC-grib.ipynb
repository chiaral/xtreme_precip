{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "from datetime import timedelta, datetime, date\n",
    "import calendar\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cape\t\t\t\t\t   sty_sfc\r\n",
      "climatology\t\t\t\t   swe_sfc\r\n",
      "correctedfiles.d\t\t\t   ta_10\r\n",
      "cprat_sfc\t\t\t\t   ta_100\r\n",
      "DJF\t\t\t\t\t   ta_30\r\n",
      "dlwrf_sfc\t\t\t\t   ta_50\r\n",
      "dswrf_sfc\t\t\t\t   tas_2m\r\n",
      "forecast\t\t\t\t   tasmax_sfc\r\n",
      "grb2_databefore1strecord.txt\t\t   tasmin_sfc\r\n",
      "grb2_large_int_byte_files.txt\t\t   tdps_sfc\r\n",
      "hfls_sfc\t\t\t\t   ts_sfc\r\n",
      "hfss_sfc\t\t\t\t   ua_10\r\n",
      "hlcy\t\t\t\t\t   ua_100\r\n",
      "huss_850\t\t\t\t   ua_10m\r\n",
      "JJA\t\t\t\t\t   ua_200\r\n",
      "originaltarfiles.d\t\t\t   ua_30\r\n",
      "pentad_climatology\t\t\t   ua_50\r\n",
      "prate_sfc\t\t\t\t   ua_850\r\n",
      "pr_sfc\t\t\t\t\t   ulwrf_sfc\r\n",
      "psl_sfc\t\t\t\t\t   uswrf_sfc\r\n",
      "rlut_toa\t\t\t\t   va_10\r\n",
      "RMM\t\t\t\t\t   va_100\r\n",
      "ROMI\t\t\t\t\t   va_10m\r\n",
      "sic_sfc\t\t\t\t\t   va_200\r\n",
      "snc_sfc\t\t\t\t\t   va_30\r\n",
      "snod_sfc\t\t\t\t   va_50\r\n",
      "soilw1_sfc\t\t\t\t   va_850\r\n",
      "soilw2_sfc\t\t\t\t   wap_500\r\n",
      "soilw3_sfc\t\t\t\t   zg_10\r\n",
      "soilw4_sfc\t\t\t\t   zg_200\r\n",
      "SON\t\t\t\t\t   zg_30\r\n",
      "sorted_grb2_EMC_GEFS_prob_files_final.txt  zg_50\r\n",
      "sorted_grb2_prob_files.txt\t\t   zg_500\r\n",
      "sort_grb2_large_int_byte_files.txt\t   zg_850\r\n",
      "stx_sfc\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Data2/SubX/EMC/GEFS/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999  2001  2003  2005\t2007  2009  2011  2013\t2015\r\n",
      "2000  2002  2004  2006\t2008  2010  2012  2014\t2016\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Data2/SubX/EMC/GEFS/tdps_sfc/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01  02\t03  04\t05  06\t07  08\t09  10\t11  12\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Data2/SubX/EMC/GEFS/tdps_sfc/1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m00.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m01.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m02.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m03.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m04.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m05.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m06.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m07.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m08.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m09.grb2\r\n",
      "tdps_sfc_GEFS_06jan1999_00z_d01_d35_m10.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m00.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m01.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m02.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m03.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m04.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m05.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m06.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m07.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m08.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m09.grb2\r\n",
      "tdps_sfc_GEFS_13jan1999_00z_d01_d35_m10.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m00.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m01.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m02.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m03.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m04.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m05.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m06.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m07.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m08.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m09.grb2\r\n",
      "tdps_sfc_GEFS_20jan1999_00z_d01_d35_m10.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m00.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m01.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m02.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m03.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m04.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m05.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m06.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m07.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m08.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m09.grb2\r\n",
      "tdps_sfc_GEFS_27jan1999_00z_d01_d35_m10.grb2\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Data2/SubX/EMC/GEFS/tdps_sfc/1999/01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is my preprocessing function, which I run on each grib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppf2_filename(ds,filename):\n",
    "    # rename coordinates\n",
    "    ds  = ds.rename({ 'lon_0':'lon', 'lat_0':'lat', 'forecast_time0':'L'})\n",
    "    # getting start time from file name\n",
    "    ts = filename.split('_')\n",
    "    whereGEFS = ts.index('GEFS')\n",
    "    dateSstr = ts[whereGEFS+1]\n",
    "    dateS = datetime.strptime(dateSstr, '%d%b%Y')\n",
    "    ds.coords['S'] = 'S', np.atleast_1d(dateS)\n",
    "    \n",
    "    # replacing L dates with integer (lead days)\n",
    "    # so that I can reduce to a fixed size\n",
    "    # BEWARE - this is a hardcoded step now, so it needs to be checked for each model\n",
    "    \n",
    "    #if you comment in the two print lines, you will see how L is read by xarray from the grib file\n",
    "#     print(ds.L)\n",
    "#     print(ds.S)\n",
    "    ds.coords['L']=np.arange(0.5,35.5)\n",
    "\n",
    "    # select US\n",
    "    ds = ds.sel(lat=slice(42,32),lon=slice(-103+360,-90+360))\n",
    "#     print(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L in this files is read by xarray like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<xarray.DataArray 'L' (L: 35)>\n",
    "array([  3600000000000,   7200000000000,  10800000000000,  14400000000000,\n",
    "        18000000000000,  21600000000000,  25200000000000,  28800000000000,\n",
    "        32400000000000,  36000000000000,  39600000000000,  43200000000000,\n",
    "        46800000000000,  50400000000000,  54000000000000,  57600000000000,\n",
    "        61200000000000,  64800000000000,  68400000000000,  72000000000000,\n",
    "        75600000000000,  79200000000000,  82800000000000,  86400000000000,\n",
    "        90000000000000,  93600000000000,  97200000000000, 100800000000000,\n",
    "       104400000000000, 108000000000000, 111600000000000, 115200000000000,\n",
    "       118800000000000, 122400000000000, 126000000000000],\n",
    "      dtype='timedelta64[ns]')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however if I use wgrib2 on one file\n",
    "`!wgrib2 -v /Data2/SubX/EMC/GEFS/cape/1999/06/cape_GEFS_02jun1999_00z_d01_d35_m00.grb2 `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get something like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` 1:0:d=1999060200:CAPE Convective Available Potential Energy [J/kg]:180-0 mb above ground:1 day fcst:ENS=low-res ctl\n",
    "2:46954:d=1999060200:CAPE Convective Available Potential Energy [J/kg]:180-0 mb above ground:2 day fcst:ENS=low-res ctl\n",
    "3:92924:d=1999060200:CAPE Convective Available Potential Energy [J/kg]:180-0 mb above ground:3 day fcst:ENS=low-res ctl\n",
    "4:139690:d=1999060200:CAPE Convective Available Potential Energy [J/kg]:180-0 mb above ground:4 day fcst:ENS=low-res ctl\n",
    "5:186582:d=1999060200:CAPE Convective Available Potential Energy [J/kg]:180-0 mb above ground:5 day fcst:ENS=low-res ctl `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure I am correctly overwriting L.\n",
    "\n",
    "I am now using L=array([  43200000000000,  129600000000000,  216000000000000,\n",
    "        302400000000000,  388800000000000,  475200000000000,\n",
    "        561600000000000,  648000000000000,  734400000000000,\n",
    "        820800000000000,  907200000000000,  993600000000000,\n",
    "       1080000000000000, 1166400000000000, 1252800000000000,\n",
    "       1339200000000000, 1425600000000000, 1512000000000000,\n",
    "       1598400000000000, 1684800000000000, 1771200000000000,\n",
    "       1857600000000000, 1944000000000000, 2030400000000000,\n",
    "       2116800000000000, 2203200000000000, 2289600000000000,\n",
    "       2376000000000000, 2462400000000000, 2548800000000000,\n",
    "       2635200000000000, 2721600000000000, 2808000000000000,\n",
    "       2894400000000000, 2980800000000000, 3067200000000000,\n",
    "       3153600000000000, 3240000000000000, 3326400000000000,\n",
    "       3412800000000000, 3499200000000000, 3585600000000000,\n",
    "       3672000000000000, 3758400000000000, 3844800000000000], etc......\n",
    "      dtype='timedelta64[ns]')\n",
    "      \n",
    "from the other models I loaded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, preprocess, append, and concatenate all the grib files. In this notebook I ran it on half of the data available.\n",
    "\n",
    "### I will comment the code below for pr_sfc. I repeat the code for each variable separiately. I could loop it for a list of varname =[ 'pr_sfc' , ....] but I preferred keeping it separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppf2_filename(ds,filename):\n",
    "    # rename coordinates\n",
    "    ds  = ds.rename({ 'lon_0':'lon', 'lat_0':'lat', 'forecast_time0':'time'})\n",
    "    # getting start time from file name\n",
    "    ts = filename.split('_')\n",
    "    print(ts)\n",
    "    whereGEFS = ts.index('GEFS')\n",
    "    dateSstr = ts[whereGEFS+1]\n",
    "    dateS = datetime.strptime(dateSstr, '%d%b%Y')\n",
    "    \n",
    "    Mvalue = np.int(float(np.asarray(ts[-1][2:4])))\n",
    "\n",
    "    ds.coords['S'] = 'S', np.atleast_1d(dateS)\n",
    "    ds.coords['M'] = 'M', np.atleast_1d(Mvalue)\n",
    "    \n",
    "    # replacing L dates with integer (lead days)\n",
    "    # so that I can reduce to a fixed size\n",
    "    # BEWARE - this is a hardcoded step now, so it needs to be checked for each model\n",
    "    \n",
    "    #if you comment in the two print lines, you will see how L is read by xarray from the grib file\n",
    "#     print(ds.L)\n",
    "#     print(ds.S)\n",
    "    ds.coords['time']=np.array([  43200000000000,  129600000000000,  216000000000000,\n",
    "        302400000000000,  388800000000000,  475200000000000,\n",
    "        561600000000000,  648000000000000,  734400000000000,\n",
    "        820800000000000,  907200000000000,  993600000000000,\n",
    "       1080000000000000, 1166400000000000, 1252800000000000,\n",
    "       1339200000000000, 1425600000000000, 1512000000000000,\n",
    "       1598400000000000, 1684800000000000, 1771200000000000,\n",
    "       1857600000000000, 1944000000000000, 2030400000000000,\n",
    "       2116800000000000, 2203200000000000, 2289600000000000,\n",
    "       2376000000000000, 2462400000000000, 2548800000000000,\n",
    "       2635200000000000, 2721600000000000, 2808000000000000,\n",
    "       2894400000000000, 2980800000000000],\n",
    "      dtype='timedelta64[ns]')\n",
    "\n",
    "    # select US\n",
    "    ds = ds.sel(lat=slice(23,50), lon=slice(230,300))\n",
    "#     print(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = xr.open_dataset('/Data2/SubX/EMC/GEFS/cape/1999/01/cape_GEFS_06jan1999_00z_d01_d35_m00.grb2',\n",
    "#                 engine='pynio')\n",
    "# ds = ppf2_filename(ds,'/Data2/SubX/EMC/GEFS/cape/1999/01/cape_GEFS_06jan1999_00z_d01_d35_m00.grb2')\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /Data2/SubX/EMC/GEFS/cape/1999/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cape\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "tas_2m\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "tdps_sfc\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "variable_names = ['cape', 'tas_2m', 'tdps_sfc', 'wap_500', 'pr_sfc']\n",
    "def ppf2_filename(ds,filename):\n",
    "    # rename coordinates\n",
    "    ds  = ds.rename({ 'lon_0':'lon', 'lat_0':'lat', 'forecast_time0':'time'})\n",
    "    # getting start time from file name\n",
    "    ts = filename.split('_')\n",
    "#     print(ts)\n",
    "    whereGEFS = ts.index('GEFS')\n",
    "    dateSstr = ts[whereGEFS+1]\n",
    "    dateS = datetime.strptime(dateSstr, '%d%b%Y')\n",
    "    \n",
    "    Mvalue = np.int(float(np.asarray(ts[-1][2:4])))\n",
    "\n",
    "    ds.coords['S'] = 'S', np.atleast_1d(dateS)\n",
    "    ds.coords['M'] = 'M', np.atleast_1d(Mvalue)\n",
    "    \n",
    "    ds.coords['time']=np.array([  43200000000000,  129600000000000,  216000000000000,\n",
    "        302400000000000,  388800000000000,  475200000000000,\n",
    "        561600000000000,  648000000000000,  734400000000000,\n",
    "        820800000000000,  907200000000000,  993600000000000,\n",
    "       1080000000000000, 1166400000000000, 1252800000000000,\n",
    "       1339200000000000, 1425600000000000, 1512000000000000,\n",
    "       1598400000000000, 1684800000000000, 1771200000000000,\n",
    "       1857600000000000, 1944000000000000, 2030400000000000,\n",
    "       2116800000000000, 2203200000000000, 2289600000000000,\n",
    "       2376000000000000, 2462400000000000, 2548800000000000,\n",
    "       2635200000000000, 2721600000000000, 2808000000000000,\n",
    "       2894400000000000, 2980800000000000],\n",
    "      dtype='timedelta64[ns]')\n",
    "\n",
    "    # select US\n",
    "    ds = ds.sel(lat=slice(50,23), lon=slice(230,300))\n",
    "#     print(ds)\n",
    "    return ds\n",
    "\n",
    "def read_netcdfs(files, dim, transform_func=None):\n",
    "    def process_one_path(path):\n",
    "        # use a context manager, to ensure the file gets closed after use\n",
    "#         print(path)\n",
    "        with xr.open_dataset(path, engine='pynio', chunks={}) as ds:\n",
    "            # transform_func should do some sort of selection or\n",
    "            # aggregation\n",
    "            if transform_func is not None:\n",
    "                ds = transform_func(ds, path)\n",
    "            # load all data from the transformed dataset, to ensure we can\n",
    "            # use it after closing each original file\n",
    "#             print(ds)\n",
    "#             ds.load()\n",
    "            return ds\n",
    "#     print(files)\n",
    "    paths = sorted(glob(files))\n",
    "#     print(paths)\n",
    "    datasets = [process_one_path(p) for p in paths]\n",
    "    \n",
    "    combined = xr.combine_by_coords(datasets)\n",
    "    combined = combined.chunk({'S':4,'M':-1})\n",
    "    return combined\n",
    "\n",
    "var_l=[]\n",
    "for ivx, ivar in enumerate(variable_names):\n",
    "    print(ivar)\n",
    "    if ivar in os.listdir('/Data2/SubX/EMC/GEFS/'):\n",
    "        c_l = []\n",
    "        for iy in np.arange(1999, 2017):#2017\n",
    "            print(iy)\n",
    "            # here we suppose we only care about the combined mean of each file;\n",
    "            # you might also use indexing operations like .sel to subset datasets\n",
    "            combined = read_netcdfs('/Data2/SubX/EMC/GEFS/'+ivar+'/'+np.str(iy)+'/*/*.grb2', dim=['S','M'],\n",
    "                                    transform_func=ppf2_filename)\n",
    "            \n",
    "            c_l.append(combined)\n",
    "    all_values = xr.concat(c_l, dim='S') \n",
    "    var_l.append(all_values)\n",
    "all_variables = xr.merge(var_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
